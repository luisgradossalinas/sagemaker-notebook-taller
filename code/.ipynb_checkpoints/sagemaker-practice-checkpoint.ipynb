{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation with Step Functions\n",
    "\n",
    "Ahora que tenemos nuestro proyecto de ciencia de datos construido, queremos implementarlo de una manera sólida y repetible. Para esto, implementaremos el ETL usando AWS Glue, y luego entrenaremos y transformaremos por lotes la entrada usando la integración de SageMaker con Amazon Step Functions.\n",
    "\n",
    "Este cuaderno lo guiará a través de este proceso paso a paso.\n",
    "\n",
    "Pero primero debe crear o configurar su propio cubo. El SDK de SageMaker es una buena forma de empezar.\n",
    "\n",
    "## 1. Upload to an S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses = sagemaker.Session()\n",
    "#your_bucket = ses.default_bucket()\n",
    "bucket = \"fashionstore-datalake-external-\" + str(account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "S3UploadFailedError",
     "evalue": "Failed to upload ../data/billing_sm.csv to fashionstore-datalake-external-971489366207/data/billing/billing_sm.csv: An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;31m# If a client error was raised, add the backwards compatibility layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# out of this and propagate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;31m# If the task is the final task, then set the TransferFuture's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, client, bucket, key, extra_args)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# Create the multipart upload.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         response = client.create_multipart_upload(\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mS3UploadFailedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7349/1278033913.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'billing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'billing_sm.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/billing_sm.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'reseller'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reseller_sm.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..data/reseller_sm.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mobject_upload_file\u001b[0;34m(self, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mtransfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \"\"\"\n\u001b[0;32m--> 318\u001b[0;31m     return self.meta.client.upload_file(\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mS3Transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransfer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         return transfer.upload_file(\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;31m# client error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             raise S3UploadFailedError(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \"Failed to upload {} to {}: {}\".format(\n\u001b[1;32m    296\u001b[0m                     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mS3UploadFailedError\u001b[0m: Failed to upload ../data/billing_sm.csv to fashionstore-datalake-external-971489366207/data/billing/billing_sm.csv: An error occurred (AccessDenied) when calling the CreateMultipartUpload operation: Access Denied"
     ]
    }
   ],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('data','billing', 'billing_sm.csv')).upload_file('../data/billing_sm.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join('data','reseller', 'reseller_sm.csv')).upload_file('..data/reseller_sm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creaando un Crawler en Glue\n",
    "\n",
    "To use this csv information in the context of a Glue ETL, first we have to create a Glue crawler pointing to the location of each file. The crawler will try to figure out the data types of each column. The safest way to do this process is to create one crawler for each table pointing to a different location.\n",
    "\n",
    "Go to the AWS Console.\n",
    "Select under Services AWS Glue.\n",
    "Or follow <a href='https://us-east-1.console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=crawlers'> this link! </a>       \n",
    "    \n",
    "\n",
    "Under crawlers Add Crawler and two crawlers: create one pointing to each S3 location (one to billing and one to reseller)\n",
    "* Crawler Name: Billing - Next\n",
    "* Crawler source type: Data Store - Next\n",
    "* Add a data store: S3, Specific Path in my Account, Navigate to your bucket and your folder Billing - Next\n",
    "* Add another data store: no - Next\n",
    "* Choose an IAM role: create an IAM role billing-crawler-role (if exists, choose the existing) - Next\n",
    "* Frequency: run on demand - Next\n",
    "* Crawler’s output: Add database implementationdb - Next\n",
    "* Finish\n",
    "\n",
    "\n",
    "Tips: \n",
    "- Make sure you name your Glue data base \"implementationdb\" \n",
    "- Make sure to point to the S3 folder containing each file, not the actual file\n",
    "- Make sure to name your db implementationdb\n",
    "- Make sure to create new roles\n",
    "\n",
    "create a new role to run the crawler. Also, don´t forget to run the crawler\n",
    "\n",
    "<img src='img/crawler1.png' style='width:400px' />\n",
    "\n",
    "<img src='img/crawler2.png' style='width:400px' />\n",
    "\n",
    "\n",
    "\n",
    "Let’s add the second crawler:\n",
    "\n",
    "* Crawler Name: Reseller - Next\n",
    "* Crawler source type: Data Store - Next\n",
    "* Add a data store: S3, Specific Path in my Account, Navigate to your bucket and your folder Reseller - Next\n",
    "* Add another data store: no - Next\n",
    "* Choose an IAM role: create an IAM role reseller-crawler-role (if exists, choose the existing) - Next\n",
    "* Frequency: run on demand - Next\n",
    "* Crawlers’s output: Select database implementationdb - Next\n",
    "* Finish\n",
    "\n",
    "\n",
    "Tips:\n",
    "- Use the same database (implementationdb) but create a different role as each crawler need to access a different folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test crawlers on Athena\n",
    "\n",
    "Go to the <a href='https://console.aws.amazon.com/athena/home?region=us-east-1#'> Athena Console </a> and hit Get Started.\n",
    "\n",
    "Under Settings in the top right corner you can configure an output path for your queries. \n",
    "You can use the following value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-646862220717/athena_results/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f's3://{your_bucket}/athena_results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you set a destination for query results, you can preview the tables created by the crawlers.\n",
    "\n",
    "<img src='img/athena1.png' style='width:500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Glue Job\n",
    "\n",
    "First of all, you need to create a role to run the Glue Job. For simplicity we are going to build a role that can be assumed by the Glue Service with administrator access. \n",
    "\n",
    "In the <a href='https://console.aws.amazon.com/athena/home?region=us-east-1#'> IAM Console </a>\n",
    "\n",
    "* Under use case select Glue\n",
    "* Under Policies Select Administrator Access\n",
    "* Name your role GlueAdmin and accept.\n",
    "\n",
    "<img src='img/gluerole1.png' style='width:500px'>\n",
    "<img src='img/gluerole2.png' style='width:500px'>\n",
    "<img src='img/gluerole3.png' style='width:500px'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now move to the <a href='https://console.aws.amazon.com/glue/home?region=us-east-1#addJob:'> Glue Job Console </a> and author a new job.\n",
    "    \n",
    "\n",
    "* Name: etlandpipeline\n",
    "* Role: Create a role named Glueadmin with AdministratorAccess (this is because we are testing)\n",
    "* Type: Python Shell\n",
    "* Glue version: Python3 (Glue Version 1.0)\n",
    "* Select A New Script Authored By you\n",
    "* Under Maximum Capacity: 1 - Next\n",
    "\n",
    "    \n",
    "Then hit “Save Job and Edit Script”\n",
    "\n",
    "You can use the following script to run your job:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = open('etlandpipeline.py', 'r').read().replace('your_bucket',your_bucket)\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the Step Function\n",
    "\n",
    "First you need to create a role that can be assumed by AWS Step Functions and have enough permissions to create and use for inference SageMaker models and run Glue Jobs. \n",
    "First, we are going to create a role that can be assumed by the service Step Functions and then we are going to modify it to add Administrator Access. You can name this role StepFunctionsAdmin\n",
    "\n",
    "\n",
    "<img src='img/iamstep.png' />\n",
    "\n",
    "Tip: In this particular case it can not be done in the same step.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Next go to the <a href='https://console.aws.amazon.com/states/home?region=us-east-1#/statemachines'> Step Functions </a> console and create a new State Machine.\n",
    "\n",
    "* Author with code snippets\n",
    "* Standard\n",
    "\n",
    "\n",
    "In the json place you can use the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "your_role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Comment\": \"Full ML Pipeline\",\n",
      "  \"StartAt\": \"Start Glue Job\",\n",
      "  \"States\": {\n",
      "    \"Start Glue Job\": {\n",
      "      \"Type\": \"Task\",\n",
      "      \"Resource\": \"arn:aws:states:::glue:startJobRun.sync\",\n",
      "      \"Parameters\": {\n",
      "        \"JobName\": \"etlandpipeline\"\n",
      "      },\n",
      "      \"Next\": \"Train model (XGBoost)\"\n",
      "    },\n",
      "    \"Train model (XGBoost)\": {\n",
      "      \"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\",\n",
      "      \"Parameters\": {\n",
      "        \"AlgorithmSpecification\": {\n",
      "          \"TrainingImage\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n",
      "          \"TrainingInputMode\": \"File\"\n",
      "        },\n",
      "        \"OutputDataConfig\": {\n",
      "          \"S3OutputPath\": \"s3://sagemaker-us-east-1-646862220717/models\"\n",
      "        },\n",
      "        \"StoppingCondition\": {\n",
      "          \"MaxRuntimeInSeconds\": 86400\n",
      "        },\n",
      "        \"ResourceConfig\": {\n",
      "          \"InstanceCount\": 1,\n",
      "          \"InstanceType\": \"ml.m4.xlarge\",\n",
      "          \"VolumeSizeInGB\": 30\n",
      "        },\n",
      "        \"RoleArn\": \"arn:aws:iam::646862220717:role/TeamRole\",\n",
      "        \"InputDataConfig\": [\n",
      "          {\n",
      "            \"DataSource\": {\n",
      "              \"S3DataSource\": {\n",
      "                \"S3DataDistributionType\": \"ShardedByS3Key\",\n",
      "                \"S3DataType\": \"S3Prefix\",\n",
      "                \"S3Uri\": \"s3://sagemaker-us-east-1-646862220717/train/train.csv\"\n",
      "              }\n",
      "            },\n",
      "            \"ChannelName\": \"train\",\n",
      "            \"ContentType\": \"text/csv\"\n",
      "          },\n",
      "          {\n",
      "            \"DataSource\": {\n",
      "              \"S3DataSource\": {\n",
      "                \"S3DataDistributionType\": \"ShardedByS3Key\",\n",
      "                \"S3DataType\": \"S3Prefix\",\n",
      "                \"S3Uri\": \"s3://sagemaker-us-east-1-646862220717/validation/validation.csv\"\n",
      "              }\n",
      "            },\n",
      "            \"ChannelName\": \"validation\",\n",
      "            \"ContentType\": \"text/csv\"\n",
      "          }\n",
      "        ],\n",
      "        \"HyperParameters\": {\n",
      "          \"objective\": \"reg:linear\",\n",
      "          \"num_round\": \"100\",\n",
      "          \"subsample\": \"0.7\",\n",
      "          \"eval_metric\": \"mae\"\n",
      "        },\n",
      "        \"TrainingJobName.$\": \"$$.Execution.Name\"\n",
      "      },\n",
      "      \"Type\": \"Task\",\n",
      "      \"Next\": \"Save Model\"\n",
      "    },\n",
      "    \"Save Model\": {\n",
      "      \"Parameters\": {\n",
      "        \"PrimaryContainer\": {\n",
      "          \"Image\": \"811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest\",\n",
      "          \"Environment\": {},\n",
      "          \"ModelDataUrl.$\": \"$.ModelArtifacts.S3ModelArtifacts\"\n",
      "        },\n",
      "        \"ExecutionRoleArn\": \"arn:aws:iam::646862220717:role/TeamRole\",\n",
      "        \"ModelName.$\": \"$.TrainingJobName\"\n",
      "      },\n",
      "      \"Resource\": \"arn:aws:states:::sagemaker:createModel\",\n",
      "      \"Type\": \"Task\",\n",
      "      \"Next\": \"Batch transform\"\n",
      "    },\n",
      "    \"Batch transform\": {\n",
      "      \"Type\": \"Task\",\n",
      "      \"Resource\": \"arn:aws:states:::sagemaker:createTransformJob.sync\",\n",
      "      \"Parameters\": {\n",
      "        \"ModelName.$\": \"$$.Execution.Name\",\n",
      "        \"TransformInput\": {\n",
      "          \"CompressionType\": \"None\",\n",
      "          \"ContentType\": \"text/csv\",\n",
      "          \"DataSource\": {\n",
      "            \"S3DataSource\": {\n",
      "              \"S3DataType\": \"S3Prefix\",\n",
      "              \"S3Uri\": \"s3://sagemaker-us-east-1-646862220717/to_predict.csv\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"TransformOutput\": {\n",
      "          \"S3OutputPath\": \"s3://sagemaker-us-east-1-646862220717/predictions\"\n",
      "        },\n",
      "        \"TransformResources\": {\n",
      "          \"InstanceCount\": 1,\n",
      "          \"InstanceType\": \"ml.m4.xlarge\"\n",
      "        },\n",
      "        \"TransformJobName.$\": \"$$.Execution.Name\"\n",
      "      },\n",
      "      \"End\": true\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "definition = open('step_function.json', 'r').read().replace('your_bucket',your_bucket).replace('your_role',your_role)\n",
    "print(definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the role that you previously created and then you can create and run your state machine. \n",
    "\n",
    "\n",
    "As you process starts running and moves thorugh each step you will be able to see the process running in each servicés console. \n",
    "\n",
    "Check <a href='https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=jobs'> Glue </a> for job logs and\n",
    "<a href='https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs'> SageMaker </a> to see the training job, the model that you created and the batch transform process. \n",
    "\n",
    "After you step function finishes the execution, you should see the graph turning to green:\n",
    "\n",
    "<img src='img/step.png' style='width:500px' />\n",
    "\n",
    "You can inspect your predictions in the predictinos folder on you bucket checking <a href='https://s3.console.aws.amazon.com/s3/home?region=us-east-1'>S3</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
